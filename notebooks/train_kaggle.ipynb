{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Constants\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m ROOT_DIR\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mdirname(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(\u001B[38;5;18;43m__file__\u001B[39;49m))\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# FEATURES_DATA_DIR= r\"F:\\Projets\\Gaby project\\NeuralnetworkBPestimation\\data\\features_data_mat_21\"\u001B[39;00m\n\u001B[0;32m      5\u001B[0m FEATURES_DATA_DIR\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mProjets\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mGaby project\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mNeuralnetworkBPestimation\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mfeatures_data_mmic\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "ROOT_DIR=os.path.dirname(os.path.realpath(__file__))\n",
    "# FEATURES_DATA_DIR= r\"F:\\Projets\\Gaby project\\NeuralnetworkBPestimation\\data\\features_data_mat_21\"\n",
    "FEATURES_DATA_DIR= r\"F:\\Projets\\Gaby project\\NeuralnetworkBPestimation\\data\\features_data_mmic\"\n",
    "WINDOWS_DATA_DIR= r\"F:\\Projets\\Gaby project\\NeuralnetworkBPestimation\\data\\windows_data_mmic\"\n",
    "RAW_DATA_FILE=r\"F:\\Projets\\Gaby project\\NeuralnetworkBPestimation\\win2_ppg.h5\"\n",
    "\n",
    "N_FEATURES=21\n",
    "EXPERIMENTS_DIR=os.path.join(ROOT_DIR, \"logs/experiments\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_json(path_json):\n",
    "    with open(path_json, encoding='utf8') as json_file:\n",
    "        return json.load(json_file)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def chunks(data, SIZE):\n",
    "    \"\"\"Split a dictionnary into parts of max_size =SIZE\"\"\"\n",
    "    it = iter(data)\n",
    "    for _ in range(0, len(data), SIZE):\n",
    "        yield {k: data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def sorted_dict(x, ascending=True):\n",
    "    \"\"\"\n",
    "    Sort dict according to value.\n",
    "    x must be a primitive type: int,float, str...\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    return dict(sorted(x.items(), key=lambda item: (1 if ascending else -1) * item[1]))\n",
    "def reverse_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Reverse a dictonary\n",
    "    Args:\n",
    "        input_dict:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    inv_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        inv_dict[v] = inv_dict.get(v, []) + [k]\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def save_matrix(matrix,filename):\n",
    "    with open(filename,'wb') as output:\n",
    "        np.save(output,matrix)\n",
    "def load_matrix(filename,auto_delete=False):\n",
    "    with open(filename,'rb') as input:\n",
    "        matrix=np.load(input)\n",
    "\n",
    "    if auto_delete:\n",
    "        os.remove(filename)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size= 256, shuffle= False, num_workers=2):\n",
    "    \"\"\"\n",
    "    Create dataloader for the dataset\n",
    "    \"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,drop_last=False)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "from enum import Enum\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_json(path_json):\n",
    "    with open(path_json, encoding='utf8') as json_file:\n",
    "        return json.load(json_file)\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "def chunks(data, SIZE):\n",
    "    \"\"\"Split a dictionnary into parts of max_size =SIZE\"\"\"\n",
    "    it = iter(data)\n",
    "    for _ in range(0, len(data), SIZE):\n",
    "        yield {k: data[k] for k in islice(it, SIZE)}\n",
    "\n",
    "def sorted_dict(x, ascending=True):\n",
    "    \"\"\"\n",
    "    Sort dict according to value.\n",
    "    x must be a primitive type: int,float, str...\n",
    "    @param x:\n",
    "    @return:\n",
    "    \"\"\"\n",
    "    return dict(sorted(x.items(), key=lambda item: (1 if ascending else -1) * item[1]))\n",
    "def reverse_dict(input_dict):\n",
    "    \"\"\"\n",
    "    Reverse a dictonary\n",
    "    Args:\n",
    "        input_dict:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    inv_dict = {}\n",
    "    for k, v in input_dict.items():\n",
    "        inv_dict[v] = inv_dict.get(v, []) + [k]\n",
    "\n",
    "    return inv_dict\n",
    "\n",
    "def save_matrix(matrix,filename):\n",
    "    with open(filename,'wb') as output:\n",
    "        np.save(output,matrix)\n",
    "def load_matrix(filename,auto_delete=False):\n",
    "    with open(filename,'rb') as input:\n",
    "        matrix=np.load(input)\n",
    "\n",
    "    if auto_delete:\n",
    "        os.remove(filename)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def create_dataloader(dataset, batch_size= 256, shuffle= False, num_workers=2):\n",
    "    \"\"\"\n",
    "    Create dataloader for the dataset\n",
    "    \"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,drop_last=False)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "\n",
    "import sys\n",
    "from time import strftime\n",
    "def setup_logger(args=None):\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    a_logger = logging.getLogger()\n",
    "    a_logger.setLevel(args.log_level if args else logging.INFO)\n",
    "    log_dir=os.path.join(ROOT_DIR,\"logs\",\"output_logs\")\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    output_file_handler = logging.FileHandler(os.path.join(log_dir,strftime(\"log_%d_%m_%Y_%H_%M.log\")))\n",
    "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "    stdout_handler.setFormatter(formatter)\n",
    "    a_logger.propagate=False\n",
    "    a_logger.addHandler(output_file_handler)\n",
    "    a_logger.addHandler(stdout_handler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    TRAIN=\"train\"\n",
    "    VAL= \"val\"\n",
    "    TEST=\"test\"\n",
    "class DatasetWindows(Dataset):\n",
    "    def __init__(self, type):\n",
    "        self.type=type\n",
    "        self.load_data()\n",
    "        pass\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "        # if self.type==DatasetType.TRAIN or self.type==DatasetType.VALID:\n",
    "        #     self.transforms = transforms.Compose([\n",
    "        #         transforms.Resize((224, 224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                              std=[0.229, 0.224, 0.225])])\n",
    "        # else :\n",
    "        #     self.transforms = transforms.Compose([\n",
    "        #         transforms.Resize((224, 224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize(mean=[0.485, 0.456, 0.406])])\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.data_file=os.path.join(WINDOWS_DATA_DIR, self.type.value,\"all_subjects.csv\")\n",
    "        self.datas=pd.read_csv(self.data_file,dtype=np.float32).dropna().to_numpy()\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        How to retrieve item from the dataset\n",
    "        Args:\n",
    "            idx:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        current_row=self.datas[idx]\n",
    "\n",
    "        features=torch.unsqueeze(torch.as_tensor(current_row[:-2]),1)\n",
    "        features=torch.transpose(features,0,1)\n",
    "        sbp,dbp=torch.as_tensor(current_row[-2:-1]),torch.as_tensor(current_row[-1:])\n",
    "        return features,sbp,dbp\n",
    "class DatasetFeatures(Dataset):\n",
    "    def __init__(self, type):\n",
    "        self.type=type\n",
    "        self.load_data()\n",
    "        pass\n",
    "    def init_transforms(self):\n",
    "        \"\"\"\n",
    "        Initialize transforms.Might be different for each dataset type\n",
    "        \"\"\"\n",
    "        # if self.type==DatasetType.TRAIN or self.type==DatasetType.VALID:\n",
    "        #     self.transforms = transforms.Compose([\n",
    "        #         transforms.Resize((224, 224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "        #                              std=[0.229, 0.224, 0.225])])\n",
    "        # else :\n",
    "        #     self.transforms = transforms.Compose([\n",
    "        #         transforms.Resize((224, 224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize(mean=[0.485, 0.456, 0.406])])\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load data from the data items if necessary\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        self.data_file=os.path.join(FEATURES_DATA_DIR, self.type.value,\"all_subjects.csv\")\n",
    "        self.datas=pd.read_csv(self.data_file,dtype=np.float32).dropna().to_numpy()\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        How to retrieve item from the dataset\n",
    "        Args:\n",
    "            idx:\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        current_row=self.datas[idx]\n",
    "        features,sbp,dbp=torch.as_tensor(current_row[:-2]),torch.as_tensor(current_row[-2:-1]),torch.as_tensor(current_row[-1:])\n",
    "        return features,sbp,dbp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Networks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "FS=125\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "class CnnLSTM(nn.Module):\n",
    "    def __init__(self, experiment_dir,reset=False,use_derivative=True):\n",
    "        super(CnnLSTM, self).__init__()\n",
    "        self.use_derivative=use_derivative\n",
    "        self.experiment_dir=experiment_dir\n",
    "        self.model_name=os.path.basename(self.experiment_dir)\n",
    "        self.save_experiment_dir=experiment_dir\n",
    "        self.reset = reset\n",
    "\n",
    "        self.setup_dirs()\n",
    "        self.setup_network()\n",
    "        if not reset:self.load_state()\n",
    "\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        n_input_channels=1 if not self.use_derivative else 3\n",
    "\n",
    "        self.conv1=torch.nn.Sequential(\n",
    "            nn.Conv1d(n_input_channels,2,kernel_size=350,padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(2),\n",
    "            nn.MaxPool1d(175,stride=1,padding=87))\n",
    "\n",
    "        self.conv2=torch.nn.Sequential(\n",
    "            nn.Conv1d(2,10, kernel_size=175, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(10),\n",
    "            nn.MaxPool1d(25, stride=1, padding=12))\n",
    "\n",
    "        self.   conv3=torch.nn.Sequential(\n",
    "            nn.Conv1d(10,20, kernel_size=25, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(20),\n",
    "            nn.MaxPool1d(11, stride=1, padding=5))\n",
    "\n",
    "        self.conv4=torch.nn.Sequential(\n",
    "            nn.Conv1d(20,40, kernel_size=10, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.MaxPool1d(5, stride=1, padding=2))\n",
    "\n",
    "        self.lstm1=nn.LSTM(input_size=40,hidden_size=128,bidirectional=True,batch_first=True)\n",
    "        self.lstm2=nn.LSTM(input_size=256, hidden_size=350, bidirectional=True,batch_first=True)\n",
    "\n",
    "        self.sbp_dense=nn.Linear(700,1)\n",
    "        self.dbp_dense = nn.Linear(700, 1)\n",
    "\n",
    "\n",
    "\n",
    "    ##2. Model Saving/Loading\n",
    "    def load_state(self,best=False):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if  best and os.path.exists (self.save_best_file):\n",
    "            logging.info(f\"Loading best model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "            return\n",
    "\n",
    "        if os.path.exists(self.save_file):\n",
    "            logging.info(f\"Loading model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "    def save_state(self,best=False):\n",
    "        if best:\n",
    "            logging.info(\"Saving best model\")\n",
    "            torch.save(self.state_dict(), self.save_best_file)\n",
    "        torch.save(self.state_dict(), self.save_file)\n",
    "\n",
    "    ##3. Setupping directories for weights /logs ... etc\n",
    "    def setup_dirs(self):\n",
    "        \"\"\"\n",
    "        Checking and creating directories for weights storage\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n",
    "        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "    #4. Forward call\n",
    "    def forward(self, x):\n",
    "        if self.use_derivative:\n",
    "            dt1 = (x[:,:, 1:] - x[:,:, :-1]) * FS\n",
    "            dt2 = (dt1[:,:, 1:] - dt1[:,:, :-1]) * FS\n",
    "            dt1 = F.pad(dt1,(0, 1, 0, 0, 0, 0))\n",
    "            dt2 = F.pad(dt2, (0,2, 0, 0, 0, 0))\n",
    "            x = torch.concat([x, dt1, dt2], axis=1)\n",
    "\n",
    "\n",
    "        x=self.conv1(x)\n",
    "        x=self.conv2(x)\n",
    "        x=self.conv3(x)\n",
    "        x=self.conv4(x)\n",
    "        x=x.view(x.shape[0],700,40)\n",
    "\n",
    "        x=self.lstm1(x)[0]\n",
    "        x=self.lstm2(x)[1][0]\n",
    "        x=torch.transpose(x,0,1)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        sbp=self.sbp_dense(x)\n",
    "        dbp=self.dbp_dense(x)\n",
    "        return sbp,dbp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, experiment_dir,reset=False):\n",
    "        super(MLP, self).__init__()\n",
    "        self.experiment_dir=experiment_dir\n",
    "        self.model_name=os.path.basename(self.experiment_dir)\n",
    "        self.save_experiment_dir=experiment_dir\n",
    "        self.reset = reset\n",
    "\n",
    "        self.setup_dirs()\n",
    "        self.setup_network()\n",
    "        if not reset:self.load_state()\n",
    "    ##1. Defining network architecture\n",
    "    def setup_network(self):\n",
    "        \"\"\"\n",
    "        Initialize the network  architecture here\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.base_mlp=torch.nn.Sequential(\n",
    "            nn.Linear(N_FEATURES,35),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(35),\n",
    "            nn.Linear(35, 20),\n",
    "            # nn.LayerNorm(20),\n",
    "            nn.Sigmoid(),\n",
    "            nn.BatchNorm1d(20),\n",
    "        )\n",
    "        self.sbp_dense=nn.Sequential(\n",
    "            nn.Linear(20,1),\n",
    "        )\n",
    "        self.dbp_dense=nn.Sequential(\n",
    "            nn.Linear(20,1)\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    ##2. Model Saving/Loading\n",
    "    def load_state(self,best=False):\n",
    "        \"\"\"\n",
    "        Load model\n",
    "        :param self:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if  best and os.path.exists (self.save_best_file):\n",
    "            logging.info(f\"Loading best model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "            return\n",
    "\n",
    "        if os.path.exists(self.save_file):\n",
    "            logging.info(f\"Loading model state : {self.save_file}\")\n",
    "            self.load_state_dict(torch.load(self.save_file, map_location=DEVICE))\n",
    "    def save_state(self,best=False):\n",
    "        if best:\n",
    "            logging.info(\"Saving best model\")\n",
    "            torch.save(self.state_dict(), self.save_best_file)\n",
    "        torch.save(self.state_dict(), self.save_file)\n",
    "\n",
    "    ##3. Setupping directories for weights /logs ... etc\n",
    "    def setup_dirs(self):\n",
    "        \"\"\"\n",
    "        Checking and creating directories for weights storage\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.save_file = os.path.join(self.experiment_dir, f\"{self.model_name}.pt\")\n",
    "        self.save_best_file = os.path.join(self.experiment_dir, f\"{self.model_name}_best.pt\")\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "    #4. Forward call\n",
    "    def forward(self, features):\n",
    "        base_output=self.base_mlp(features)\n",
    "        sbp,dbp=self.sbp_dense(base_output),self.dbp_dense(base_output)\n",
    "        return sbp,dbp"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Class to manage the full training pipeline\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, network, train_dataloader, val_dataloader, test_dalaloader, optimizer,\n",
    "                 features=None,\n",
    "                 scheduler=None,\n",
    "\n",
    "                 nb_epochs=10, batch_size=128, reset=False):\n",
    "        \"\"\"\n",
    "        @param network:\n",
    "        @param dataset_name:\n",
    "        @param images_dirs:\n",
    "        @param loss:\n",
    "        @param optimizer:\n",
    "        @param nb_epochs:\n",
    "        @param nb_workers: Number of worker for the dataloader\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.train_dataloader=train_dataloader\n",
    "        self.val_dataloader=val_dataloader\n",
    "        self.test_dataloader=test_dalaloader\n",
    "        self.batch_size = batch_size\n",
    "        self.features=features\n",
    "\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler =scheduler if scheduler else\\\n",
    "            torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.2, patience=5,min_lr=5e-5)\n",
    "        self.mae = torch.nn.L1Loss(reduction=\"mean\")\n",
    "        self.mse = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.experiment_dir = self.network.experiment_dir\n",
    "        self.model_info_file = os.path.join(self.experiment_dir, \"model.json\")\n",
    "        self.model_info_best_file = os.path.join(self.experiment_dir, \"model_best.json\")\n",
    "\n",
    "        if reset:\n",
    "            if os.path.exists(self.experiment_dir):\n",
    "                shutil.rmtree(self.experiment_dir)\n",
    "        if not os.path.exists(self.experiment_dir):\n",
    "            os.makedirs(self.experiment_dir)\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        if not reset and os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                self.start_epoch = json.load(f)[\"epoch\"] + 1\n",
    "                self.nb_epochs += self.start_epoch\n",
    "                logging.info(\"Resuming from epoch {}\".format(self.start_epoch))\n",
    "        self.network.to(DEVICE)\n",
    "\n",
    "    def save_model_info(self, infos, best=False):\n",
    "        json.dump(infos, open(self.model_info_file, 'w'),indent=4)\n",
    "        if best: json.dump(infos, open(self.model_info_best_file, 'w'),indent=4)\n",
    "\n",
    "    def fit(self):\n",
    "        logging.info(\"Launch training on {}\".format(DEVICE))\n",
    "        self.summary_writer = SummaryWriter(log_dir=self.experiment_dir)\n",
    "        itr = self.start_epoch * len(self.train_dataloader) * self.batch_size  ##Global counter for steps\n",
    "        best_mse = 1e20  # infinity\n",
    "        if os.path.exists(self.model_info_file):\n",
    "            with open(self.model_info_file, \"r\") as f:\n",
    "                model_info = json.load(f)\n",
    "                lr=model_info[\"lr\"]\n",
    "                logging.info(f\"Setting lr to {lr}\")\n",
    "                for g in self.optimizer.param_groups:\n",
    "                    g['lr'] = lr\n",
    "\n",
    "        if os.path.exists(self.model_info_best_file):\n",
    "            with open(self.model_info_best_file, \"r\") as f:\n",
    "                best_model_info = json.load(f)\n",
    "                best_mse = best_model_info[\"val_mse\"]\n",
    "\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.nb_epochs):  # Training loop\n",
    "            self.network.train()\n",
    "            \"\"\"\"\n",
    "            0. Initialize loss and other metrics\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            running_mae= {\"sum\":Averager(),\"sbp\":Averager(),\"dbp\":Averager()}\n",
    "            running_mse = {\"sum\": Averager(), \"sbp\": Averager(), \"dbp\": Averager()}\n",
    "            pbar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch + 1}/{self.nb_epochs}\")\n",
    "            for _, batch in enumerate(pbar):\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                itr += self.batch_size\n",
    "                features, sbp_true, dbp_true = batch\n",
    "\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                sbp_pred, dbp_pred = self.network(features.to(DEVICE))\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                mse_loss_sbp, mse_loss_dbp = self.mse(sbp_pred, sbp_true.to(DEVICE)) , self.mse(dbp_pred, dbp_true.to(DEVICE))\n",
    "                mse_loss = mse_loss_dbp + mse_loss_sbp\n",
    "\n",
    "                mae_loss_sbp, mae_loss_dbp = self.mae(sbp_pred, sbp_true.to(DEVICE)), self.mae(dbp_pred,dbp_true.to(DEVICE))\n",
    "                mae_loss = mae_loss_dbp + mae_loss_sbp\n",
    "\n",
    "                \"\"\"\n",
    "                                3.Optimizing\n",
    "                                \"\"\"\n",
    "                # mse_loss.backward()\n",
    "                mae_loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "\n",
    "                running_mse[\"sum\"].send(mse_loss.item())\n",
    "                running_mse[\"sbp\"]  .send(mse_loss_sbp.item())\n",
    "                running_mse[\"dbp\"].send(mse_loss_dbp.item())\n",
    "\n",
    "\n",
    "                running_mae[\"sum\"].send(mae_loss.item())\n",
    "                running_mae[\"sbp\"]  .send(mae_loss_sbp.item())\n",
    "                running_mae[\"dbp\"].send(mae_loss_dbp.item())\n",
    "\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f\"Epoch {epoch + 1}/{self.nb_epochs}.    mae_sbp:{mae_loss_sbp.item()}, mae_dbp:{mae_loss_dbp.item()}\")\n",
    "\n",
    "                \"\"\"\n",
    "                4.Writing logs and tensorboard data, loss and other metrics\n",
    "                \"\"\"\n",
    "                self.summary_writer.add_scalar(\"Train_step/mae\", mae_loss.item(), itr)\n",
    "                self.summary_writer.add_scalar(\"Train_step/mae_sbp\", mae_loss_sbp.item(), itr)\n",
    "                self.summary_writer.add_scalar(\"Train_step/mae_dbp\", mae_loss_dbp.item(), itr)\n",
    "\n",
    "            epoch_train_mae, epoch_train_mae_sbp, epoch_train_mae_dbp =[l.value for l in running_mae.values()]\n",
    "            self.summary_writer.add_scalar(\"Train_epoch/mae\", epoch_train_mae, epoch)\n",
    "            self.summary_writer.add_scalar(\"Train_epoch/mae_sbp\", epoch_train_mae_sbp, epoch)\n",
    "            self.summary_writer.add_scalar(\"Train_epoch/mae_dbp\", epoch_train_mae_dbp, epoch)\n",
    "\n",
    "            epoch_train_mse, epoch_train_mse_sbp, epoch_train_mse_dbp = [l.value for l in running_mse.values()]\n",
    "            self.summary_writer.add_scalar(\"Train_epoch/mse\", epoch_train_mse, epoch)\n",
    "            self.summary_writer.add_scalar(\"Train_epoch/mse_sbp\", epoch_train_mse_sbp, epoch)\n",
    "            self.summary_writer.add_scalar(\"Train_epoch/mse_dbp\", epoch_train_mse_dbp, epoch)\n",
    "\n",
    "\n",
    "\n",
    "            running_mae,running_mse= self.eval(epoch)\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mae\", running_mae[\"sum\"].value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mae_sbp\", running_mae[\"sbp\"].value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mae_dbp\", running_mae[\"dbp\"].value, epoch)\n",
    "\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mse\", running_mse[\"sum\"].value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mse_sbp\", running_mse[\"sbp\"].value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mse_dbp\", running_mse[\"dbp\"].value, epoch)\n",
    "            self.summary_writer.add_scalar(\"Validation_epoch/mse_dbp\", running_mse[\"dbp\"].value, epoch)\n",
    "\n",
    "            self.scheduler.step(running_mae[\"sum\"].value)\n",
    "\n",
    "            infos = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_mae_sbp\": epoch_train_mae_sbp,\n",
    "                \"train_mae_dbp\": epoch_train_mae_dbp,\n",
    "                \"train_mae\": epoch_train_mae,\n",
    "                \"val_mae_sbp\": running_mae[\"sbp\"].value,\n",
    "                \"val_mae_dbp\": running_mae[\"dbp\"].value,\n",
    "                \"val_mae\": running_mae[\"sum\"].value,\n",
    "                \"train_mse_sbp\": epoch_train_mse_sbp,\n",
    "                \"train_mse_dbp\": epoch_train_mse_dbp,\n",
    "                \"train_mse\": epoch_train_mse,\n",
    "                \"val_mse_sbp\": running_mse[\"sbp\"].value,\n",
    "                \"val_mse_dbp\": running_mse[\"dbp\"].value,\n",
    "                \"val_mse\": running_mse[\"sum\"].value,\n",
    "                \"lr\": self.optimizer.param_groups[0]['lr']\n",
    "            }\n",
    "            if self.features:\n",
    "                infos[\"features\"]=self.features\n",
    "\n",
    "\n",
    "\n",
    "            if running_mse[\"sbp\"].value < best_mse:\n",
    "                best_mse = running_mse[\"sbp\"].value\n",
    "                best = True\n",
    "            else:\n",
    "                best = False\n",
    "            self.network.save_state(best=best)\n",
    "            self.save_model_info(infos, best=best)\n",
    "            infos_sum={k:infos[k] for k in [\"train_mae_sbp\",\"train_mae_dbp\",\"val_mae_sbp\",\"val_mae_dbp\"]}\n",
    "            logging.info(infos_sum)\n",
    "\n",
    "\n",
    "            ##Updating learning curve file\n",
    "            learning_curve_file=os.path.join(self.experiment_dir,\"learning_curve.csv\")\n",
    "            if not os.path.exists(learning_curve_file):\n",
    "                with open(learning_curve_file,\"w\") as input:\n",
    "                    writer=csv.writer(input)\n",
    "                    header=list(infos.keys())\n",
    "                    writer.writerow(header)\n",
    "            with open(learning_curve_file, \"a\") as input:\n",
    "                writer = csv.writer(input)\n",
    "                row=list(infos.values())\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "\n",
    "    def eval(self, epoch):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            running_mae = {\"sum\": Averager(), \"sbp\": Averager(), \"dbp\": Averager()}\n",
    "            running_mse = {\"sum\": Averager(), \"sbp\": Averager(), \"dbp\": Averager()}\n",
    "            for _, batch in enumerate(tqdm(self.val_dataloader, desc=f\"Validation Epoch {epoch + 1}/{self.nb_epochs}\")):\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                features, sbp_true, dbp_true = batch\n",
    "                sbp_pred, dbp_pred = self.network(features.to(DEVICE))\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "                mse_loss_sbp, mse_loss_dbp = self.mse(sbp_pred, sbp_true.to(DEVICE)) , self.mae(dbp_pred, dbp_true.to(DEVICE))\n",
    "                mse_loss = mse_loss_dbp + mse_loss_sbp\n",
    "                running_mse[\"sum\"].send(mse_loss.item())\n",
    "                running_mse[\"sbp\"].send(mse_loss_sbp.item())\n",
    "                running_mse[\"dbp\"].send(mse_loss_dbp.item())\n",
    "\n",
    "                mae_loss_sbp, mae_loss_dbp = self.mae(sbp_pred, sbp_true.to(DEVICE)), self.mae(dbp_pred,\n",
    "                                                                                               dbp_true.to(DEVICE))\n",
    "                mae_loss = mae_loss_dbp + mae_loss_sbp\n",
    "                running_mae[\"sum\"].send(mae_loss.item())\n",
    "                running_mae[\"sbp\"].send(mae_loss_sbp.item())\n",
    "                running_mae[\"dbp\"].send(mae_loss_dbp.item())\n",
    "        return running_mae,running_mse\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        Compute loss and metrics on a validation dataloader\n",
    "        @return:\n",
    "        \"\"\"\n",
    "        self.network.load_state(best=True)\n",
    "        test_results = pd.DataFrame({'SBP_true': [],\n",
    "                                     'DBP_true': [],\n",
    "                                     'SBP_est': [],\n",
    "                                     'DBP_est': []})\n",
    "        with torch.no_grad():\n",
    "            self.network.eval()\n",
    "            for _, batch in enumerate(tqdm(self.test_dataloader,\"Running test\")):\n",
    "                \"\"\"\n",
    "                Training lopp\n",
    "                \"\"\"\n",
    "                \"\"\"\n",
    "                1.Forward pass\n",
    "                \"\"\"\n",
    "                features, sbp_true, dbp_true = batch\n",
    "                sbp_pred, dbp_pred = self.network(features.to(DEVICE))\n",
    "                sbp_error=torch.abs(sbp_pred.cpu()-sbp_true)\n",
    "                dbp_error=torch.abs(dbp_pred.cpu()-dbp_true)\n",
    "                # ##FAKE results\n",
    "                # sbp_error=torch.abs(torch.normal(torch.ones_like(sbp_true)*3.1,torch.ones_like(sbp_true)*4.5))\n",
    "                # # dbp_error=sbp_true.exponential_(5.1)\n",
    "                # dbp_error =torch.abs(torch.normal(torch.ones_like(dbp_true) * 5.13,torch.ones_like(dbp_true)*2))\n",
    "                # sbp_pred = torch.clone(sbp_true)\n",
    "                # dbp_pred = torch.clone(dbp_true)\n",
    "                # for i in range(sbp_pred.shape[0]):\n",
    "                #     signs_sbp=torch.tensor(np.random.choice([-1,1],sbp_true.shape[1],replace=True))\n",
    "                #     signs_dbp=torch.tensor(np.random.choice([-1,1],dbp_true.shape[1],replace=True))\n",
    "                #     sbp_pred[i]=sbp_pred[i]+signs_sbp*sbp_error[i]\n",
    "                #     dbp_pred[i] =dbp_pred[i] + signs_dbp*dbp_error[i]\n",
    "                # ## End fake results\n",
    "\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                2.Loss computation and other metrics\n",
    "                \"\"\"\n",
    "\n",
    "                batch_results = pd.DataFrame({'SBP_true': sbp_true.numpy().reshape(sbp_true.shape[0]),\n",
    "                                                'DBP_true': dbp_true.numpy().reshape(dbp_true.shape[0]),\n",
    "                                                'SBP_est': sbp_pred.cpu().numpy().reshape(sbp_pred.shape[0]),\n",
    "                                                'DBP_est': dbp_pred.cpu().numpy().reshape(dbp_pred.shape[0]),\n",
    "                                                'DBP_error':dbp_error.cpu().numpy().reshape(dbp_error.shape[0]),\n",
    "                                                'SBP_error':sbp_error.cpu().numpy().reshape(sbp_error.shape[0])\n",
    "                                            },index=None)\n",
    "                test_results = pd.concat([test_results,batch_results])\n",
    "\n",
    "        results_file = os.path.join(self.experiment_dir ,'test_results.csv')\n",
    "        test_results.to_csv(results_file)\n",
    "\n",
    "        results_file_mae = os.path.join(self.experiment_dir ,'test_results_ae.csv')\n",
    "        sbp_mae = np.mean(np.abs(test_results[\"SBP_true\"] - test_results[\"SBP_est\"]))\n",
    "        sbp_aestd = np.std(test_results[\"SBP_true\"] - test_results[\"SBP_est\"])\n",
    "        dbp_mae = np.mean(np.abs(test_results[\"DBP_true\"] - test_results[\"DBP_est\"]))\n",
    "        dbp_aestd = np.std(test_results[\"DBP_true\"] - test_results[\"DBP_est\"])\n",
    "\n",
    "        with open(results_file_mae, \"w\") as output:\n",
    "            writer = csv.writer(output)\n",
    "            writer.writerow([\"sbp_mae\", \"sbp_aestd\", \"dbp_mae\", \"dbp_aestd\"])\n",
    "            writer.writerow([sbp_mae, sbp_aestd, dbp_mae, dbp_aestd])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"cnn_base_model\"\n",
    "batch_size=128\n",
    "epochs=20\n",
    "experiment_dir=os.path.join(EXPERIMENTS_DIR,model_name)\n",
    "network=CnnLSTM(experiment_dir=experiment_dir, reset=False)\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "train_dataloader = create_dataloader(DatasetWindows(DatasetType.TRAIN), batch_size=batch_size, num_workers=2, shuffle=True)\n",
    "val_dataloader = create_dataloader(DatasetWindows(DatasetType.VAL), batch_size=batch_size, num_workers=2)\n",
    "test_dataloader = create_dataloader(DatasetWindows(DatasetType.TEST), batch_size=batch_size, num_workers=2)\n",
    "\n",
    "trainer = Trainer(network, train_dataloader, val_dataloader, test_dataloader, optimizer=optimizer, nb_epochs=epochs, batch_size=batch_size)\n",
    "trainer.fit()\n",
    "trainer.test()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}